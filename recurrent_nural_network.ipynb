{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "recurrent_nural_network.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vishnubanna/stocktrader/blob/master/recurrent_nural_network.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "whjSY8rVQqxZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf \n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "from collections import deque\n",
        "from sklearn import preprocessing\n",
        "import datetime as dt\n",
        "\n",
        "import pandas as pd\n",
        "from pandas_datareader import data as pdr\n",
        "import fix_yahoo_finance as yf\n",
        "yf.pdr_override()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3X_qLwyWRmYb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def getData(ticker, number_of_days = 10000, days_ago = 0):\n",
        "    try:\n",
        "        print(ticker)\n",
        "        start = dt.datetime.today() - dt.timedelta(days = number_of_days)\n",
        "        end = dt.datetime.today() - dt.timedelta(days = days_ago)\n",
        "        df = pdr.get_data_yahoo(ticker, start, end)\n",
        "        print(df.tail())\n",
        "        return df\n",
        "    except:\n",
        "        pass\n",
        "    \n",
        "def dataSpecify(df):\n",
        "    df['e50MAvg'] = df['Close'].ewm(span = 50, adjust = False, min_periods = 0).mean()\n",
        "    df['e26MAvg'] = df['Close'].ewm(span = 26, adjust = False, min_periods = 0).mean()\n",
        "    df['e12MAvg'] = df['Close'].ewm(span = 12, adjust = False, min_periods = 0).mean()\n",
        "\n",
        "    df['MACD'] = df['e12MAvg'] - df['e26MAvg']\n",
        "\n",
        "    df['MFlowMult'] = ((df.Close - df.Low) - (df.High - df.Close))/(df.High - df.Low)\n",
        "    df['MFlowVol'] = (df['Volume'])*df['MFlowMult']\n",
        "    df['ADL'] = df['MFlowVol'].values.cumsum()\n",
        "    #df['ADL'] = df['ADL'].shift(1)\n",
        "    df.fillna(0, inplace = True)\n",
        "    df['e50AAvg'] = df['ADL'].ewm(span = 50, adjust = False, min_periods = 0).mean()\n",
        "    df['e10MAvg'] = df['ADL'].ewm(span = 10, adjust = False, min_periods = 0).mean()\n",
        "    df['e3MAvg'] = df['ADL'].ewm(span = 3, adjust = False, min_periods = 0).mean()\n",
        "\n",
        "    df['Chakin'] = df['e3MAvg'] - df['e10MAvg']\n",
        "    \n",
        "    #df = df.drop(['e26MAvg', 'e12MAvg', 'High', 'Low', 'Adj Close', 'e3MAvg', 'e10MAvg'], axis = 1)\n",
        "    df = df[['Open', 'Close','e26MAvg' , 'e50MAvg', 'MACD', 'ADL', 'Chakin', 'High', 'Low']]\n",
        "    print(df.head(), df.tail())\n",
        "    return df\n",
        "\n",
        "df = getData('AAPL')\n",
        "df = dataSpecify(df)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XqiLgWFqWqDs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "SEQ_LEN = 60 #how many sequential data points correspond to one prediction. in out case we are using the last 60 points to to make a prediction. so the last 60 days including today can be used to predict tommorow\n",
        "FUTURE_PERIOD_PREDICT = 10 # how many days in advance do you want to predict\n",
        "DATA_TO_PREDICT = 'e50MAvg' # what do you want to predict \n",
        "\n",
        "def classify(current_value, future_value):\n",
        "    if float(current_value) <= float(future_value): # rather than predict the future price, if the furture is better or higher, get 1 or hold\n",
        "        return 1\n",
        "    else: # else 0 sell\n",
        "        return 0\n",
        "    \n",
        "    \n",
        "def process_df(df, scale = 0):\n",
        "    #scales = []\n",
        "    #scale = 0\n",
        "    # normalize the colomns: all have different magnitude of values, this reduces the magnitude down by using percent change from point to point, reduces so all numbers in df are within the same range or a similar range\n",
        "    for col in df.columns: \n",
        "        # don't change the target colomn\n",
        "        if col != 'targets':\n",
        "            #all have different magnitude of values, this reduces the magnitude down by using percent change from point to point, reduces so all numbers in df are within the same range or a similar range\n",
        "            df[col] = df[col].pct_change()\n",
        "            df[col].replace([np.inf, -np.inf], np.nan, inplace = True)\n",
        "            #df.dropna(inplace = True)\n",
        "    \n",
        "    df.dropna(inplace = True)\n",
        "#     for col in df.columns:\n",
        "#         if col != 'targets':\n",
        "#             df[col] = preprocessing.scale(df[col].values)\n",
        "    target = np.asarray([df['targets'].values])\n",
        "    #print(target.shape)\n",
        "    \n",
        "    #print(df.tail())\n",
        "    df = df.drop(['targets'], axis = 1)\n",
        "    \n",
        "    if scale == 0:\n",
        "        scale = preprocessing.RobustScaler().fit(df.values)\n",
        "        scaled_df = scale.transform(df.values)\n",
        "    else:\n",
        "        scaled_df = scale.transform(df.values)\n",
        "    #print(scaled_df.shape)   \n",
        "    scaled_df = np.concatenate((scaled_df, target.T), axis = 1)\n",
        "    #print(scaled_df)\n",
        "  \n",
        "    sequential_data = [] # our data points will go here \n",
        "    prev_days = deque(maxlen = SEQ_LEN) #so it compiles 60 days of data into one list, so on the day today it puts the past 60 days including today, for yesterday it dos the past 60 day unil yesterday including yerstery, it stops\n",
        "        # when it cal no longer populate a sinle list with 60 days of data \n",
        " \n",
        "    for i in scaled_df:\n",
        "        prev_days.append([n for n in i[:-1]])\n",
        "        if len(prev_days) == 60: \n",
        "            sequential_data.append([np.array(prev_days), i[-1]])\n",
        "       \n",
        "    print((sequential_data[0]))\n",
        "    print((sequential_data[len(sequential_data)-1]))\n",
        "    \n",
        "    #shuffle the sequenced arrays;; to prevent the model from memeorizing the data set, and thinking pushing the training set and testing set to not represent each other \n",
        "    random.shuffle(sequential_data)\n",
        "    \n",
        "    #balence the data set:: make sure therea re the ame number of buys and sells\n",
        "    buys = []\n",
        "    sells = []\n",
        "    \n",
        "    for data, target in sequential_data:\n",
        "        if target == 1:\n",
        "            buys.append([data, target])\n",
        "        else:\n",
        "            sells.append([data, target])\n",
        "    \n",
        "    random.shuffle(buys)\n",
        "    random.shuffle(sells)\n",
        "    \n",
        "    \n",
        "    size = min(len(buys), len(sells))\n",
        "    \n",
        "    \n",
        "    buys = buys[:size]\n",
        "    sells = sells[:size]\n",
        "    \n",
        "    print(len(buys), len(sells))\n",
        "    \n",
        "    fullset = buys + sells\n",
        "    random.shuffle(fullset)\n",
        "    \n",
        "    X = []\n",
        "    Y = []\n",
        "    \n",
        "    for data, target in fullset:\n",
        "        X.append(data)\n",
        "        Y.append(target)\n",
        "                \n",
        "    return np.array(X), np.array(Y), scale\n",
        "\n",
        "\n",
        "def process_df2(df, scale = 0):\n",
        "    #scales = []\n",
        "    #scale = 0\n",
        "    # normalize the colomns: all have different magnitude of values, this reduces the magnitude down by using percent change from point to point, reduces so all numbers in df are within the same range or a similar range\n",
        "    for col in df.columns: \n",
        "        # don't change the target colomn\n",
        "        if col != 'targets':\n",
        "            #all have different magnitude of values, this reduces the magnitude down by using percent change from point to point, reduces so all numbers in df are within the same range or a similar range\n",
        "            df[col] = df[col].pct_change()\n",
        "            df[col].replace([np.inf, -np.inf], np.nan, inplace = True)\n",
        "            #df.dropna(inplace = True)\n",
        "    \n",
        "    df.dropna(inplace = True)\n",
        "#     for col in df.columns:\n",
        "#         if col != 'targets':\n",
        "#             df[col] = preprocessing.scale(df[col].values)\n",
        "    target = np.asarray([df['targets'].values])\n",
        "    #print(target.shape)\n",
        "    \n",
        "    #print(df.tail())\n",
        "    df = df.drop(['targets'], axis = 1)\n",
        "    \n",
        "    if scale == 0:\n",
        "        scale = preprocessing.RobustScaler().fit(df.values)\n",
        "        scaled_df = scale.transform(df.values)\n",
        "    else:\n",
        "        scaled_df = scale.transform(df.values)\n",
        "    #print(scaled_df.shape)   \n",
        "    scaled_df = np.concatenate((scaled_df, target.T), axis = 1)\n",
        "    #print(scaled_df)\n",
        "  \n",
        "    sequential_data = [] # our data points will go here \n",
        "    prev_days = deque(maxlen = SEQ_LEN) #so it compiles 60 days of data into one list, so on the day today it puts the past 60 days including today, for yesterday it dos the past 60 day unil yesterday including yerstery, it stops\n",
        "        # when it cal no longer populate a sinle list with 60 days of data \n",
        " \n",
        "    for i in scaled_df:\n",
        "        prev_days.append([n for n in i[:-1]])\n",
        "        if len(prev_days) == 60: \n",
        "            sequential_data.append([np.array(prev_days), i[-1]])\n",
        "       \n",
        "    #print((sequential_data))\n",
        "    \n",
        "    #shuffle the sequenced arrays;; to prevent the model from memeorizing the data set, and thinking pushing the training set and testing set to not represent each other \n",
        "    #random.shuffle(sequential_data)\n",
        "    \n",
        "    #balence the data set:: make sure therea re the ame number of buys and sells\n",
        "    buys = []\n",
        "    sells = []\n",
        "    \n",
        "    for data, target in sequential_data:\n",
        "        if target == 1:\n",
        "            buys.append([data, target])\n",
        "        else:\n",
        "            sells.append([data, target])\n",
        "    \n",
        "    #random.shuffle(buys)\n",
        "    #random.shuffle(sells)\n",
        "    \n",
        "    \n",
        "    #size = min(len(buys), len(sells))\n",
        "    \n",
        "    \n",
        "    #buys = buys[:size]\n",
        "    #sells = sells[:size]\n",
        "    \n",
        "    print(len(buys), len(sells))\n",
        "    \n",
        "    fullset = buys + sells\n",
        "    random.shuffle(fullset)\n",
        "    \n",
        "    X = []\n",
        "    Y = []\n",
        "    \n",
        "    for data, target in sequential_data:\n",
        "        X.append(data)\n",
        "        Y.append(target)\n",
        "                \n",
        "    return np.array(X), np.array(Y), scale\n",
        "\n",
        "            \n",
        "    \n",
        "\n",
        "df['future'] = df[DATA_TO_PREDICT].shift(-FUTURE_PERIOD_PREDICT)\n",
        "df['targets'] = list(map(classify, df[DATA_TO_PREDICT], df['future']))\n",
        "df.dropna(inplace = True)\n",
        "df = df.drop(['future'], axis = 1)\n",
        "#df = df.drop(['0'], axis = 1) #figure out how\n",
        "print(df.head())\n",
        "print(df.tail())\n",
        "\n",
        "\n",
        "#split the training data and the testing data:: we ahve to sort and take a chunk because if we randomized data(if it sequential), the data will have a very similar counterpart of itself inside the training set. \n",
        "#at which point it will, have high accuracy even if it not truly that accurate\n",
        "# with sequencial data only\n",
        "\n",
        "# they should be in order, but just make sure\n",
        "dates = sorted(df.index.values)\n",
        "last_5pct_data = dates[-int(0.05*len(dates))]\n",
        "print(last_5pct_data)\n",
        "\n",
        "train_df = df[(df.index < last_5pct_data)] # all data point with data less than the last 5pct data = training, all data grater = first 5 pct\n",
        "test_df = df[(df.index >= last_5pct_data)]\n",
        "\n",
        "print(train_df.tail())\n",
        "\n",
        "#process_df(train_df)\n",
        "x_train, y_train, scale = process_df(train_df)\n",
        "x_test, y_test, scale2 = process_df(test_df, scale = scale) \n",
        "x_test1, y_test1, scale3 = process_df2(test_df, scale = scale)\n",
        "\n",
        "\n",
        "\n",
        "#print(x_test, y_test)\n",
        "\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I6vOs0OwGh4y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pip install tensorflow==2.0.0-alpha0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qmXv979hFFSF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow.keras as ks\n",
        "from tensorflow.keras.layers import Dense, LSTM, Dropout, InputLayer, BatchNormalization\n",
        "\n",
        "layers = [\n",
        "    InputLayer(input_shape = (x_train.shape[1:] )),\n",
        "    LSTM(units = 50, activation = 'tanh', return_sequences = True), \n",
        "    Dropout(0.2),\n",
        "    BatchNormalization(),\n",
        "    LSTM(units = 50, activation = 'tanh', return_sequences = True), \n",
        "    Dropout(0.2),\n",
        "    BatchNormalization(),\n",
        "    LSTM(units = 50, activation = 'tanh'), \n",
        "    Dropout(0.2),\n",
        "    BatchNormalization(),\n",
        "    Dense(units = 32, activation = 'relu'), \n",
        "    Dropout(0.2),\n",
        "    Dense(units = 2, activation = 'softmax'), \n",
        "    \n",
        "]\n",
        "model = ks.Sequential(layers)\n",
        "\n",
        "opt = ks.optimizers.Adam(lr = 0.001, decay = 0.000001)\n",
        "\n",
        "model.compile(loss = ks.losses.SparseCategoricalCrossentropy(), \n",
        "             optimizer = opt, \n",
        "             metrics = ['accuracy'])\n",
        "\n",
        "model.fit(x_train, y_train, batch_size = 30, epochs = 7, validation_data = (x_test, y_test))\n",
        "\n",
        "model.predict(x_test, verbose = 0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UO7aeXI0dhIv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test = model.predict(x_test1, verbose = 0)\n",
        "print(y_test1)\n",
        "test = test.tolist()\n",
        "\n",
        "decision = ['buy' if (max(tes[0],tes[1]) == tes[1]) else 'sell' for tes in test]\n",
        "test = ['buy' if y == 1 else 'sell' for y in y_test1]\n",
        "\n",
        "print(decision)\n",
        "print(test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hLBDVuceEock",
        "colab_type": "text"
      },
      "source": [
        "                Open     Close   e50MAvg      MACD       ADL    Chakin  targets\n",
        "Date                                                                           \n",
        "2019-04-25 -0.002556 -0.009075  0.003351 -0.026472 -0.000771 -0.364350        0\n",
        "2019-04-26 -0.009331 -0.004774  0.003006 -0.046782  0.000454 -0.146574        1\n",
        "2019-04-29 -0.002440  0.001517  0.002944 -0.045174 -0.000304 -0.301522        1\n",
        "2019-04-30 -0.006556 -0.019256  0.002011 -0.110670 -0.000601 -0.664775        1\n",
        "2019-05-01  0.033586  0.049086  0.003947  0.065975 -0.001767 -4.386830        0"
      ]
    }
  ]
}