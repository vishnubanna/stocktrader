{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Stock trend prediction NURAL NET TEST.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vishnubanna/stocktrader/blob/master/Stock_trend_prediction_NURAL_NET_TEST.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4-U2PIPkBExV",
        "colab_type": "code",
        "outputId": "1052fbe2-b4d0-4d0e-ca3c-50548d247ef7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 360
        }
      },
      "source": [
        "pip install tensorflow==2.0.0-alpha0"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow==2.0.0-alpha0 in /usr/local/lib/python3.6/dist-packages (2.0.0a0)\n",
            "Requirement already satisfied: google-pasta>=0.1.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-alpha0) (0.1.7)\n",
            "Requirement already satisfied: tb-nightly<1.14.0a20190302,>=1.14.0a20190301 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-alpha0) (1.14.0a20190301)\n",
            "Requirement already satisfied: tf-estimator-nightly<1.14.0.dev2019030116,>=1.14.0.dev2019030115 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-alpha0) (1.14.0.dev2019030115)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-alpha0) (1.0.9)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-alpha0) (0.2.2)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-alpha0) (0.33.4)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-alpha0) (1.15.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-alpha0) (3.7.1)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-alpha0) (0.7.1)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-alpha0) (0.8.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.14.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-alpha0) (1.16.3)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-alpha0) (1.1.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-alpha0) (1.0.7)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-alpha0) (1.12.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.14.0a20190302,>=1.14.0a20190301->tensorflow==2.0.0-alpha0) (3.1.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.14.0a20190302,>=1.14.0a20190301->tensorflow==2.0.0-alpha0) (0.15.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow==2.0.0-alpha0) (41.0.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow==2.0.0-alpha0) (2.8.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "siqEZ86KBWl_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "import time\n",
        "import datetime as dt\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.dates as mdates\n",
        "from sklearn import preprocessing, svm\n",
        "from sklearn.model_selection import cross_validate, train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "from pandas_datareader import data as pdr\n",
        "import fix_yahoo_finance as yf\n",
        "yf.pdr_override()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "THRsJDAQCHJs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_data(ticker):\n",
        "  try:\n",
        "    print(ticker)\n",
        "    #start = dt.datetime(1950, 1, 1)\n",
        "    start = dt.datetime(2000, 1, 1)\n",
        "    #start = dt.datetime(2010, 1, 1)\n",
        "    end = dt.datetime.today() - dt.timedelta(days = 1)\n",
        "    df = pdr.get_data_yahoo(ticker, start, end)\n",
        "    \n",
        "    while emptychek(df):\n",
        "        df = pdr.get_data_yahoo(ticker, start, end)\n",
        "    \n",
        "#     print((df.dropna(inplace = True)).empty)\n",
        "    \n",
        "#     while df.empty: \n",
        "#         print('here')\n",
        "#         df = pdr.get_data_yahoo(ticker, start, end)\n",
        "        \n",
        "#     empty = True\n",
        "#     if empty: \n",
        "#         try: \n",
        "#             print(df.head())\n",
        "#             empty = False\n",
        "#         except:\n",
        "#             empty = True \n",
        "    \n",
        "#     print(empty)\n",
        "        \n",
        "    print(df.head())\n",
        "    print(df.tail())\n",
        "    return(df)\n",
        "  except:\n",
        "    pass\n",
        "\n",
        "def emptychek(df):\n",
        "    empty = True\n",
        "    if empty: \n",
        "        try: \n",
        "            print(df.head())\n",
        "            empty = False\n",
        "        except:\n",
        "            empty = True \n",
        "    \n",
        "    print(empty)\n",
        "    return\n",
        "  \n",
        "def graphData(stock):\n",
        "    try:\n",
        "        df = stock\n",
        "        df.reset_index(inplace = True)\n",
        "        fig = plt.figure()\n",
        "        ax1 = plt.subplot(2,1,1)\n",
        "        ax1.plot(df.Date, df.Open)\n",
        "        ax1.plot(df.Date, df.High)\n",
        "        ax1.plot(df.Date, df.Low)\n",
        "        ax1.plot(df.Date, df.Close)\n",
        "        ax1.grid(True)\n",
        "    \n",
        "        ax2 = plt.subplot(2,1,2)\n",
        "        ax2.bar(df.Date,df.Volume)\n",
        "    \n",
        "        ax1.xaxis.set_major_locator(mticker.MaxNLocator(10))\n",
        "        ax1.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))\n",
        "    \n",
        "        for label in ax1.xaxis.get_tickerlabels():\n",
        "            label.set_rotation(45)     \n",
        "        return\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "def timeSeries(df):\n",
        "    df['f01'] = df['Adj Close'].shift(-1)\n",
        "    df['f02'] = df['Adj Close'].shift(-2)\n",
        "    df['f03'] = df['Adj Close'].shift(-3)\n",
        "    df['f04'] = df['Adj Close'].shift(-4)\n",
        "    df['f05'] = df['Adj Close'].shift(-5)\n",
        "    df['f06'] = df['Adj Close'].shift(-6)\n",
        "    df['f07'] = df['Adj Close'].shift(-7)\n",
        "    \n",
        "    df = df[['Close', 'f01', 'f02', 'f03', 'f04', 'f05', 'f06', 'f07']]\n",
        "    \n",
        "    print(df.head())\n",
        "    print(df.tail())\n",
        "    df.dropna(inplace = True)\n",
        "    \n",
        "    return df\n",
        "\n",
        "def dataSpecify(df):\n",
        "    print(df.head(), df.tail())\n",
        "    \n",
        "    df['50 MAvg'] = df['Close'].rolling(window = 50, min_periods = 0).mean()\n",
        "    df['20 MAvg'] = df['Close'].rolling(window = 20, min_periods = 0).mean()\n",
        "    df['10 MAvg'] = df['Close'].rolling(window = 10, min_periods = 0).mean()\n",
        "    df['VolumeMAvg'] = df['Volume'].rolling(window = 50, min_periods = 0).mean()\n",
        "    df['volitility1'] = (df['Close']-df['50 MAvg'])/df['Close'] * 100\n",
        "\n",
        "    df['vshift'] = df['volitility1'].shift(1)\n",
        "    df['volSlope'] = df['volitility1'] - df['vshift']\n",
        "\n",
        "    df['50MAshift'] = df['50 MAvg'].shift(1)\n",
        "    df['50MASlope'] = (df['50 MAvg'] - df['50MAshift'])\n",
        "\n",
        "    df['% change day'] = (df['Open'] - df['Close']) / df['Open'] * 100\n",
        "    df['% daily volit'] = (df['High'] - df['Close']) / df['High'] * 100\n",
        "    \n",
        "    df['cShift'] = df['Close'].shift(1)\n",
        "    #\n",
        "    df['%volit'] = (df['cShift'] - df['Close']) / df['cShift'] * 100\n",
        "    \n",
        "    df.fillna(0, inplace = True)\n",
        "    \n",
        "    \n",
        "    print(df['%volit'].head(30))\n",
        "    \n",
        "    \n",
        "    #df = df[['Close', '50 MAvg', 'Volume', 'volitility1', '% daily volit','% change day' ,'VolumeMAvg', '%volit']]\n",
        "    #df = df[['Close', '50 MAvg', 'Volume', 'volitility1', '% daily volit','% change day' ,'VolumeMAvg', '%volit']]\n",
        "    df = df[['Close', 'Open', '50 MAvg', 'Volume', 'volitility1', '% daily volit','% change day' ,'VolumeMAvg', '%volit']]\n",
        "    print(df.head(), df.tail())\n",
        "    \n",
        "    return df\n",
        "\n",
        "def certainty_testing(df):\n",
        "    close = np.ndarray.flatten(df.VolumeMAvg.values)\n",
        "    test = (df.Volume.values).flatten('F')[:len(close)]\n",
        "\n",
        "    print(test, len(test))\n",
        "    print(len(close))\n",
        "\n",
        "    certainty = []\n",
        "\n",
        "    for i in range(len(close)):\n",
        "        volume = test[i]\n",
        "        vavg = close[i]\n",
        "\n",
        "        if volume > vavg:\n",
        "            certainty.append(1)\n",
        "        else:\n",
        "            certainty.append(0)\n",
        "    \n",
        "    \n",
        "    print(volume)\n",
        "    print(test[1249])\n",
        "    print(vavg)\n",
        "    print(close[1249])\n",
        "    print(certainty)\n",
        "    \n",
        "    return certainty\n",
        "\n",
        "def train_test_sets(df, scaling = False, days = 1):\n",
        "    forecast_col = 'Close'\n",
        "    df.fillna(-99999, inplace = True)\n",
        "    forecast_out = days\n",
        "\n",
        "    df['Label'] = df[forecast_col].shift(-forecast_out)\n",
        "    print(df.tail())\n",
        "\n",
        "    #X = np.array(df.drop(['Label', 'Close'], axis = 1))\n",
        "    X = np.array(df.drop(['Label', '%volit'], axis = 1))\n",
        "    if scaling:\n",
        "        X = preprocessing.scale(X)\n",
        "    X_lately = X[-forecast_out:] #forcastout = 30, (-)30 from the bottom \n",
        "    X = X[:-forecast_out]\n",
        "\n",
        "    df.dropna(inplace = True)\n",
        "    y = np.array(df['Label'])\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)\n",
        "\n",
        "    print(X_train)\n",
        "    \n",
        "    return X_train, X_test, y_train, y_test, X_lately\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rvyp7JbSCglp",
        "colab_type": "code",
        "outputId": "a6e37812-88eb-45cd-a881-9ba72a115b8e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        }
      },
      "source": [
        "df = get_data('AAPL')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "AAPL\n",
            "[*********************100%***********************]  1 of 1 downloaded\n",
            "                Open      High       Low     Close  Adj Close     Volume\n",
            "Date                                                                    \n",
            "2000-01-03  3.745536  4.017857  3.631696  3.997768   2.655498  133949200\n",
            "2000-01-04  3.866071  3.950893  3.613839  3.660714   2.431611  128094400\n",
            "2000-01-05  3.705357  3.948661  3.678571  3.714286   2.467196  194580400\n",
            "2000-01-06  3.790179  3.821429  3.392857  3.392857   2.253689  191993200\n",
            "2000-01-07  3.446429  3.607143  3.410714  3.553571   2.360442  115183600\n",
            "False\n",
            "                Open      High       Low     Close  Adj Close     Volume\n",
            "Date                                                                    \n",
            "2000-01-03  3.745536  4.017857  3.631696  3.997768   2.655498  133949200\n",
            "2000-01-04  3.866071  3.950893  3.613839  3.660714   2.431611  128094400\n",
            "2000-01-05  3.705357  3.948661  3.678571  3.714286   2.467196  194580400\n",
            "2000-01-06  3.790179  3.821429  3.392857  3.392857   2.253689  191993200\n",
            "2000-01-07  3.446429  3.607143  3.410714  3.553571   2.360442  115183600\n",
            "                  Open        High  ...   Adj Close    Volume\n",
            "Date                                ...                      \n",
            "2019-05-22  184.660004  185.710007  ...  182.779999  29748600\n",
            "2019-05-23  179.800003  180.539993  ...  179.660004  36529700\n",
            "2019-05-24  180.199997  182.139999  ...  178.970001  23714700\n",
            "2019-05-28  178.919998  180.589996  ...  178.229996  27948200\n",
            "2019-05-29  176.419998  179.350006  ...  177.380005  28452000\n",
            "\n",
            "[5 rows x 6 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ubB51JNRXOS",
        "colab_type": "code",
        "outputId": "8be19e57-b2fb-4039-c3b5-a6ac090426cc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1105
        }
      },
      "source": [
        "data = dataSpecify(df)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                Open      High       Low     Close  Adj Close     Volume\n",
            "Date                                                                    \n",
            "2000-01-03  3.745536  4.017857  3.631696  3.997768   2.655498  133949200\n",
            "2000-01-04  3.866071  3.950893  3.613839  3.660714   2.431611  128094400\n",
            "2000-01-05  3.705357  3.948661  3.678571  3.714286   2.467196  194580400\n",
            "2000-01-06  3.790179  3.821429  3.392857  3.392857   2.253689  191993200\n",
            "2000-01-07  3.446429  3.607143  3.410714  3.553571   2.360442  115183600                   Open        High  ...   Adj Close    Volume\n",
            "Date                                ...                      \n",
            "2019-05-22  184.660004  185.710007  ...  182.779999  29748600\n",
            "2019-05-23  179.800003  180.539993  ...  179.660004  36529700\n",
            "2019-05-24  180.199997  182.139999  ...  178.970001  23714700\n",
            "2019-05-28  178.919998  180.589996  ...  178.229996  27948200\n",
            "2019-05-29  176.419998  179.350006  ...  177.380005  28452000\n",
            "\n",
            "[5 rows x 6 columns]\n",
            "Date\n",
            "2000-01-03     0.000000\n",
            "2000-01-04     8.431055\n",
            "2000-01-05    -1.463430\n",
            "2000-01-06     8.653857\n",
            "2000-01-07    -4.736834\n",
            "2000-01-10     1.758794\n",
            "2000-01-11     5.115078\n",
            "2000-01-12     5.997313\n",
            "2000-01-13   -10.967748\n",
            "2000-01-14    -3.811386\n",
            "2000-01-18    -3.484754\n",
            "2000-01-19    -2.525556\n",
            "2000-01-20    -6.510241\n",
            "2000-01-21     1.927313\n",
            "2000-01-24     4.547993\n",
            "2000-01-25    -5.647066\n",
            "2000-01-26     1.837423\n",
            "2000-01-27     0.170179\n",
            "2000-01-28     7.613634\n",
            "2000-01-31    -2.091025\n",
            "2000-02-01     3.373494\n",
            "2000-02-02     1.433907\n",
            "2000-02-03    -4.554071\n",
            "2000-02-04    -4.537213\n",
            "2000-02-07    -5.613429\n",
            "2000-02-08    -0.712332\n",
            "2000-02-09     1.958671\n",
            "2000-02-10    -0.776915\n",
            "2000-02-11     4.185001\n",
            "2000-02-14    -6.494248\n",
            "Name: %volit, dtype: float64\n",
            "               Close      Open   50 MAvg  ...  % change day   VolumeMAvg    %volit\n",
            "Date                                      ...                                     \n",
            "2000-01-03  3.997768  3.745536  3.997768  ...     -6.734203  133949200.0  0.000000\n",
            "2000-01-04  3.660714  3.866071  3.829241  ...      5.311775  131021800.0  8.431055\n",
            "2000-01-05  3.714286  3.705357  3.790923  ...     -0.240975  152208000.0 -1.463430\n",
            "2000-01-06  3.392857  3.790179  3.691406  ...     10.482935  162154300.0  8.653857\n",
            "2000-01-07  3.553571  3.446429  3.663839  ...     -3.108783  152760160.0 -4.736834\n",
            "\n",
            "[5 rows x 9 columns]                  Close        Open  ...  VolumeMAvg    %volit\n",
            "Date                                ...                      \n",
            "2019-05-22  182.779999  184.660004  ...  30489506.0  2.047163\n",
            "2019-05-23  179.660004  179.800003  ...  30599450.0  1.706967\n",
            "2019-05-24  178.970001  180.199997  ...  30602154.0  0.384060\n",
            "2019-05-28  178.229996  178.919998  ...  30380260.0  0.413480\n",
            "2019-05-29  177.380005  176.419998  ...  30424904.0  0.476907\n",
            "\n",
            "[5 rows x 9 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QpUuTH8yRs1s",
        "colab_type": "code",
        "outputId": "5e5c0dc2-783d-444f-b469-91d6d167d565",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        }
      },
      "source": [
        "ts = timeSeries(df)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "               Close       f01       f02  ...       f05       f06       f07\n",
            "Date                                      ...                              \n",
            "2000-01-03  3.997768  2.431611  2.467196  ...  2.318927  2.200312  2.068352\n",
            "2000-01-04  3.660714  2.467196  2.253689  ...  2.200312  2.068352  2.295204\n",
            "2000-01-05  3.714286  2.253689  2.360442  ...  2.068352  2.295204  2.382682\n",
            "2000-01-06  3.392857  2.360442  2.318927  ...  2.295204  2.382682  2.465714\n",
            "2000-01-07  3.553571  2.318927  2.200312  ...  2.382682  2.465714  2.527986\n",
            "\n",
            "[5 rows x 8 columns]\n",
            "                 Close         f01         f02  ...  f05  f06  f07\n",
            "Date                                            ...               \n",
            "2019-05-22  182.779999  179.660004  178.970001  ...  NaN  NaN  NaN\n",
            "2019-05-23  179.660004  178.970001  178.229996  ...  NaN  NaN  NaN\n",
            "2019-05-24  178.970001  178.229996  177.380005  ...  NaN  NaN  NaN\n",
            "2019-05-28  178.229996  177.380005         NaN  ...  NaN  NaN  NaN\n",
            "2019-05-29  177.380005         NaN         NaN  ...  NaN  NaN  NaN\n",
            "\n",
            "[5 rows x 8 columns]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:84: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ohVGbppCpDh",
        "colab_type": "code",
        "outputId": "671b0e4a-fb72-4883-c130-0f9339b9a501",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 717
        }
      },
      "source": [
        "#certainty_t = certainty_testing(df)\n",
        "#ct = pd.DataFrame(certainty_t)\n",
        "\n",
        "#X_train, X_test, y_train, y_test, X_lately = train_test_sets(ts, scaling = False, days = 8)\n",
        "X_train, X_test, y_train, y_test, X_lately = train_test_sets(data, scaling = True, days = 1)\n",
        "print(X_lately)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/pandas/core/frame.py:4034: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
            "  downcast=downcast, **kwargs)\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:155: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/data.py:172: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.\n",
            "  warnings.warn(\"Numerical issues were encountered \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "                 Close        Open   50 MAvg  ...  VolumeMAvg    %volit       Label\n",
            "Date                                          ...                                  \n",
            "2019-05-22  182.779999  184.660004  195.8866  ...  30489506.0  2.047163  179.660004\n",
            "2019-05-23  179.660004  179.800003  195.8456  ...  30599450.0  1.706967  178.970001\n",
            "2019-05-24  178.970001  180.199997  195.7504  ...  30602154.0  0.384060  178.229996\n",
            "2019-05-28  178.229996  178.919998  195.5926  ...  30380260.0  0.413480  177.380005\n",
            "2019-05-29  177.380005  176.419998  195.3798  ...  30424904.0  0.476907         NaN\n",
            "\n",
            "[5 rows x 10 columns]\n",
            "[[-0.41282348 -0.4051654  -0.42406739 ...  0.25226403  0.71515084\n",
            "   0.04810571]\n",
            " [-0.85619817 -0.85592525 -0.86031522 ...  0.11117283  0.34999178\n",
            "   0.04180481]\n",
            " [ 0.87950251  0.91404276  1.03279991 ...  0.37089152  0.89456945\n",
            "  -1.04093769]\n",
            " ...\n",
            " [-0.83222271 -0.83348103 -0.83241628 ... -0.18240173 -0.47774235\n",
            "   0.81677955]\n",
            " [-0.32325805 -0.32051544 -0.32154829 ...  0.27836041  0.23240497\n",
            "   0.54409789]\n",
            " [-0.90797782 -0.90802389 -0.90689918 ...  1.09019614  0.49050176\n",
            "  -0.52476809]]\n",
            "[[ 2.10687471  2.09109087  2.46900282 -0.90119474 -0.79482587 -0.20275629\n",
            "  -0.24764526 -1.16516985]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:165: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SswsLdakHsJ3",
        "colab_type": "code",
        "outputId": "537d7ddf-816f-49b2-9391-750aeb8fafd0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3451
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow.keras as ks\n",
        "\n",
        "\n",
        "\n",
        "layers = [\n",
        "    ks.layers.InputLayer(input_shape = (len(X_train[1]), )),\n",
        "    ks.layers.Dense(units = 64, activation = ks.activations.relu),\n",
        "    #ks.layers.Dropout(0.25),\n",
        "    ks.layers.Dense(units = 64, activation = ks.activations.relu),\n",
        "    #ks.layers.Dropout(0.25),\n",
        "    ks.layers.Dense(units = 1)\n",
        "]\n",
        "model = ks.Sequential(layers)\n",
        "\n",
        "model.compile(optimizer = ks.optimizers.Adam(),\n",
        "              loss = ks.losses.MeanSquaredError(),\n",
        "              metrics = [ks.metrics.MeanAbsoluteError(), ks.metrics.MeanSquaredError()]\n",
        ")\n",
        "\n",
        "model.fit(x = X_train, y = y_train, batch_size = 20, epochs = 100)\n",
        "model.evaluate(x = X_test, y = y_test, batch_size = 1)\n",
        "\n",
        "prediction = model.predict(X_lately, verbose = 0)\n",
        "\n",
        "print(prediction)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "3904/3904 [==============================] - 0s 98us/sample - loss: 3248.3356 - mean_absolute_error: 35.4359 - mean_squared_error: 3248.3342\n",
            "Epoch 2/100\n",
            "3904/3904 [==============================] - 0s 65us/sample - loss: 113.9717 - mean_absolute_error: 7.8288 - mean_squared_error: 113.9716\n",
            "Epoch 3/100\n",
            "3904/3904 [==============================] - 0s 65us/sample - loss: 50.6821 - mean_absolute_error: 5.3881 - mean_squared_error: 50.6821\n",
            "Epoch 4/100\n",
            "3904/3904 [==============================] - 0s 64us/sample - loss: 24.0032 - mean_absolute_error: 3.6796 - mean_squared_error: 24.0032\n",
            "Epoch 5/100\n",
            "3904/3904 [==============================] - 0s 65us/sample - loss: 12.5375 - mean_absolute_error: 2.6064 - mean_squared_error: 12.5375\n",
            "Epoch 6/100\n",
            "3904/3904 [==============================] - 0s 66us/sample - loss: 7.7419 - mean_absolute_error: 2.0164 - mean_squared_error: 7.7419\n",
            "Epoch 7/100\n",
            "3904/3904 [==============================] - 0s 64us/sample - loss: 5.5647 - mean_absolute_error: 1.6926 - mean_squared_error: 5.5647\n",
            "Epoch 8/100\n",
            "3904/3904 [==============================] - 0s 65us/sample - loss: 4.3745 - mean_absolute_error: 1.4733 - mean_squared_error: 4.3745\n",
            "Epoch 9/100\n",
            "3904/3904 [==============================] - 0s 65us/sample - loss: 3.7413 - mean_absolute_error: 1.3430 - mean_squared_error: 3.7413\n",
            "Epoch 10/100\n",
            "3904/3904 [==============================] - 0s 66us/sample - loss: 3.2296 - mean_absolute_error: 1.2235 - mean_squared_error: 3.2296\n",
            "Epoch 11/100\n",
            "3904/3904 [==============================] - 0s 64us/sample - loss: 2.8144 - mean_absolute_error: 1.1306 - mean_squared_error: 2.8144\n",
            "Epoch 12/100\n",
            "3904/3904 [==============================] - 0s 65us/sample - loss: 2.6104 - mean_absolute_error: 1.0816 - mean_squared_error: 2.6104\n",
            "Epoch 13/100\n",
            "3904/3904 [==============================] - 0s 65us/sample - loss: 2.4330 - mean_absolute_error: 1.0150 - mean_squared_error: 2.4330\n",
            "Epoch 14/100\n",
            "3904/3904 [==============================] - 0s 65us/sample - loss: 2.4498 - mean_absolute_error: 1.0091 - mean_squared_error: 2.4498\n",
            "Epoch 15/100\n",
            "3904/3904 [==============================] - 0s 65us/sample - loss: 2.2336 - mean_absolute_error: 0.9463 - mean_squared_error: 2.2336\n",
            "Epoch 16/100\n",
            "3904/3904 [==============================] - 0s 64us/sample - loss: 2.1317 - mean_absolute_error: 0.9247 - mean_squared_error: 2.1317\n",
            "Epoch 17/100\n",
            "3904/3904 [==============================] - 0s 66us/sample - loss: 2.0993 - mean_absolute_error: 0.8968 - mean_squared_error: 2.0993\n",
            "Epoch 18/100\n",
            "3904/3904 [==============================] - 0s 66us/sample - loss: 2.1990 - mean_absolute_error: 0.9096 - mean_squared_error: 2.1990\n",
            "Epoch 19/100\n",
            "3904/3904 [==============================] - 0s 65us/sample - loss: 2.0476 - mean_absolute_error: 0.8818 - mean_squared_error: 2.0476\n",
            "Epoch 20/100\n",
            "3904/3904 [==============================] - 0s 65us/sample - loss: 2.0952 - mean_absolute_error: 0.8854 - mean_squared_error: 2.0952\n",
            "Epoch 21/100\n",
            "3904/3904 [==============================] - 0s 67us/sample - loss: 1.9614 - mean_absolute_error: 0.8569 - mean_squared_error: 1.9614\n",
            "Epoch 22/100\n",
            "3904/3904 [==============================] - 0s 65us/sample - loss: 1.9703 - mean_absolute_error: 0.8704 - mean_squared_error: 1.9703\n",
            "Epoch 23/100\n",
            "3904/3904 [==============================] - 0s 64us/sample - loss: 2.0017 - mean_absolute_error: 0.8433 - mean_squared_error: 2.0017\n",
            "Epoch 24/100\n",
            "3904/3904 [==============================] - 0s 65us/sample - loss: 1.9039 - mean_absolute_error: 0.8277 - mean_squared_error: 1.9039\n",
            "Epoch 25/100\n",
            "3904/3904 [==============================] - 0s 66us/sample - loss: 2.0501 - mean_absolute_error: 0.8644 - mean_squared_error: 2.0501\n",
            "Epoch 26/100\n",
            "3904/3904 [==============================] - 0s 64us/sample - loss: 1.9362 - mean_absolute_error: 0.8299 - mean_squared_error: 1.9362\n",
            "Epoch 27/100\n",
            "3904/3904 [==============================] - 0s 66us/sample - loss: 1.8887 - mean_absolute_error: 0.8274 - mean_squared_error: 1.8887\n",
            "Epoch 28/100\n",
            "3904/3904 [==============================] - 0s 65us/sample - loss: 2.0367 - mean_absolute_error: 0.8693 - mean_squared_error: 2.0367\n",
            "Epoch 29/100\n",
            "3904/3904 [==============================] - 0s 65us/sample - loss: 1.9659 - mean_absolute_error: 0.8306 - mean_squared_error: 1.9659\n",
            "Epoch 30/100\n",
            "3904/3904 [==============================] - 0s 64us/sample - loss: 2.2904 - mean_absolute_error: 0.9159 - mean_squared_error: 2.2904\n",
            "Epoch 31/100\n",
            "3904/3904 [==============================] - 0s 68us/sample - loss: 1.8284 - mean_absolute_error: 0.7951 - mean_squared_error: 1.8284\n",
            "Epoch 32/100\n",
            "3904/3904 [==============================] - 0s 66us/sample - loss: 1.8940 - mean_absolute_error: 0.8136 - mean_squared_error: 1.8940\n",
            "Epoch 33/100\n",
            "3904/3904 [==============================] - 0s 65us/sample - loss: 2.0881 - mean_absolute_error: 0.8579 - mean_squared_error: 2.0881\n",
            "Epoch 34/100\n",
            "3904/3904 [==============================] - 0s 64us/sample - loss: 1.9077 - mean_absolute_error: 0.8208 - mean_squared_error: 1.9077\n",
            "Epoch 35/100\n",
            "3904/3904 [==============================] - 0s 66us/sample - loss: 1.9894 - mean_absolute_error: 0.8288 - mean_squared_error: 1.9894\n",
            "Epoch 36/100\n",
            "3904/3904 [==============================] - 0s 67us/sample - loss: 2.0157 - mean_absolute_error: 0.8443 - mean_squared_error: 2.0157\n",
            "Epoch 37/100\n",
            "3904/3904 [==============================] - 0s 63us/sample - loss: 1.9098 - mean_absolute_error: 0.8069 - mean_squared_error: 1.9098\n",
            "Epoch 38/100\n",
            "3904/3904 [==============================] - 0s 68us/sample - loss: 1.8308 - mean_absolute_error: 0.7811 - mean_squared_error: 1.8308\n",
            "Epoch 39/100\n",
            "3904/3904 [==============================] - 0s 65us/sample - loss: 1.9018 - mean_absolute_error: 0.8142 - mean_squared_error: 1.9018\n",
            "Epoch 40/100\n",
            "3904/3904 [==============================] - 0s 65us/sample - loss: 2.1195 - mean_absolute_error: 0.8601 - mean_squared_error: 2.1195\n",
            "Epoch 41/100\n",
            "3904/3904 [==============================] - 0s 65us/sample - loss: 1.8510 - mean_absolute_error: 0.7935 - mean_squared_error: 1.8510\n",
            "Epoch 42/100\n",
            "3904/3904 [==============================] - 0s 66us/sample - loss: 1.8847 - mean_absolute_error: 0.7965 - mean_squared_error: 1.8847\n",
            "Epoch 43/100\n",
            "3904/3904 [==============================] - 0s 65us/sample - loss: 1.9648 - mean_absolute_error: 0.8148 - mean_squared_error: 1.9648\n",
            "Epoch 44/100\n",
            "3904/3904 [==============================] - 0s 68us/sample - loss: 1.7511 - mean_absolute_error: 0.7703 - mean_squared_error: 1.7511\n",
            "Epoch 45/100\n",
            "3904/3904 [==============================] - 0s 63us/sample - loss: 1.9542 - mean_absolute_error: 0.8158 - mean_squared_error: 1.9542\n",
            "Epoch 46/100\n",
            "3904/3904 [==============================] - 0s 64us/sample - loss: 1.8468 - mean_absolute_error: 0.8155 - mean_squared_error: 1.8468\n",
            "Epoch 47/100\n",
            "3904/3904 [==============================] - 0s 65us/sample - loss: 1.8240 - mean_absolute_error: 0.7770 - mean_squared_error: 1.8240\n",
            "Epoch 48/100\n",
            "3904/3904 [==============================] - 0s 64us/sample - loss: 2.0741 - mean_absolute_error: 0.8509 - mean_squared_error: 2.0741\n",
            "Epoch 49/100\n",
            "3904/3904 [==============================] - 0s 65us/sample - loss: 2.1631 - mean_absolute_error: 0.8719 - mean_squared_error: 2.1631\n",
            "Epoch 50/100\n",
            "3904/3904 [==============================] - 0s 64us/sample - loss: 1.7656 - mean_absolute_error: 0.7694 - mean_squared_error: 1.7656\n",
            "Epoch 51/100\n",
            "3904/3904 [==============================] - 0s 68us/sample - loss: 1.8762 - mean_absolute_error: 0.8070 - mean_squared_error: 1.8762\n",
            "Epoch 52/100\n",
            "3904/3904 [==============================] - 0s 65us/sample - loss: 1.9713 - mean_absolute_error: 0.8353 - mean_squared_error: 1.9713\n",
            "Epoch 53/100\n",
            "3904/3904 [==============================] - 0s 64us/sample - loss: 1.8565 - mean_absolute_error: 0.8007 - mean_squared_error: 1.8565\n",
            "Epoch 54/100\n",
            "3904/3904 [==============================] - 0s 64us/sample - loss: 2.1198 - mean_absolute_error: 0.8530 - mean_squared_error: 2.1198\n",
            "Epoch 55/100\n",
            "3904/3904 [==============================] - 0s 67us/sample - loss: 1.8542 - mean_absolute_error: 0.8040 - mean_squared_error: 1.8542\n",
            "Epoch 56/100\n",
            "3904/3904 [==============================] - 0s 65us/sample - loss: 2.0237 - mean_absolute_error: 0.8343 - mean_squared_error: 2.0237\n",
            "Epoch 57/100\n",
            "3904/3904 [==============================] - 0s 65us/sample - loss: 1.8796 - mean_absolute_error: 0.8081 - mean_squared_error: 1.8796\n",
            "Epoch 58/100\n",
            "3904/3904 [==============================] - 0s 67us/sample - loss: 2.0239 - mean_absolute_error: 0.8553 - mean_squared_error: 2.0239\n",
            "Epoch 59/100\n",
            "3904/3904 [==============================] - 0s 66us/sample - loss: 1.9378 - mean_absolute_error: 0.8051 - mean_squared_error: 1.9378\n",
            "Epoch 60/100\n",
            "3904/3904 [==============================] - 0s 64us/sample - loss: 1.8280 - mean_absolute_error: 0.7812 - mean_squared_error: 1.8280\n",
            "Epoch 61/100\n",
            "3904/3904 [==============================] - 0s 69us/sample - loss: 1.8033 - mean_absolute_error: 0.7669 - mean_squared_error: 1.8033\n",
            "Epoch 62/100\n",
            "3904/3904 [==============================] - 0s 64us/sample - loss: 1.8874 - mean_absolute_error: 0.8009 - mean_squared_error: 1.8874\n",
            "Epoch 63/100\n",
            "3904/3904 [==============================] - 0s 65us/sample - loss: 1.8500 - mean_absolute_error: 0.7987 - mean_squared_error: 1.8500\n",
            "Epoch 64/100\n",
            "3904/3904 [==============================] - 0s 68us/sample - loss: 1.8132 - mean_absolute_error: 0.7813 - mean_squared_error: 1.8132\n",
            "Epoch 65/100\n",
            "3904/3904 [==============================] - 0s 68us/sample - loss: 1.9502 - mean_absolute_error: 0.8114 - mean_squared_error: 1.9502\n",
            "Epoch 66/100\n",
            "3904/3904 [==============================] - 0s 70us/sample - loss: 1.8582 - mean_absolute_error: 0.7978 - mean_squared_error: 1.8582\n",
            "Epoch 67/100\n",
            "3904/3904 [==============================] - 0s 69us/sample - loss: 1.8705 - mean_absolute_error: 0.7854 - mean_squared_error: 1.8705\n",
            "Epoch 68/100\n",
            "3904/3904 [==============================] - 0s 70us/sample - loss: 1.7939 - mean_absolute_error: 0.8027 - mean_squared_error: 1.7939\n",
            "Epoch 69/100\n",
            "3904/3904 [==============================] - 0s 70us/sample - loss: 1.9942 - mean_absolute_error: 0.8298 - mean_squared_error: 1.9942\n",
            "Epoch 70/100\n",
            "3904/3904 [==============================] - 0s 70us/sample - loss: 1.8709 - mean_absolute_error: 0.7970 - mean_squared_error: 1.8709\n",
            "Epoch 71/100\n",
            "3904/3904 [==============================] - 0s 69us/sample - loss: 1.8452 - mean_absolute_error: 0.7950 - mean_squared_error: 1.8452\n",
            "Epoch 72/100\n",
            "3904/3904 [==============================] - 0s 69us/sample - loss: 1.9611 - mean_absolute_error: 0.8048 - mean_squared_error: 1.9611\n",
            "Epoch 73/100\n",
            "3904/3904 [==============================] - 0s 70us/sample - loss: 1.8928 - mean_absolute_error: 0.7843 - mean_squared_error: 1.8928\n",
            "Epoch 74/100\n",
            "3904/3904 [==============================] - 0s 70us/sample - loss: 1.7950 - mean_absolute_error: 0.7859 - mean_squared_error: 1.7950\n",
            "Epoch 75/100\n",
            "3904/3904 [==============================] - 0s 68us/sample - loss: 1.7637 - mean_absolute_error: 0.7516 - mean_squared_error: 1.7637\n",
            "Epoch 76/100\n",
            "3904/3904 [==============================] - 0s 68us/sample - loss: 1.9035 - mean_absolute_error: 0.8068 - mean_squared_error: 1.9035\n",
            "Epoch 77/100\n",
            "3904/3904 [==============================] - 0s 69us/sample - loss: 1.8820 - mean_absolute_error: 0.8179 - mean_squared_error: 1.8820\n",
            "Epoch 78/100\n",
            "3904/3904 [==============================] - 0s 72us/sample - loss: 1.8199 - mean_absolute_error: 0.7947 - mean_squared_error: 1.8199\n",
            "Epoch 79/100\n",
            "3904/3904 [==============================] - 0s 70us/sample - loss: 1.9542 - mean_absolute_error: 0.8295 - mean_squared_error: 1.9542\n",
            "Epoch 80/100\n",
            "3904/3904 [==============================] - 0s 66us/sample - loss: 1.8188 - mean_absolute_error: 0.7709 - mean_squared_error: 1.8188\n",
            "Epoch 81/100\n",
            "3904/3904 [==============================] - 0s 69us/sample - loss: 1.8891 - mean_absolute_error: 0.7996 - mean_squared_error: 1.8891\n",
            "Epoch 82/100\n",
            "3904/3904 [==============================] - 0s 69us/sample - loss: 1.8455 - mean_absolute_error: 0.7715 - mean_squared_error: 1.8455\n",
            "Epoch 83/100\n",
            "3904/3904 [==============================] - 0s 67us/sample - loss: 1.9855 - mean_absolute_error: 0.8313 - mean_squared_error: 1.9855\n",
            "Epoch 84/100\n",
            "3904/3904 [==============================] - 0s 64us/sample - loss: 1.9279 - mean_absolute_error: 0.8255 - mean_squared_error: 1.9279\n",
            "Epoch 85/100\n",
            "3904/3904 [==============================] - 0s 65us/sample - loss: 1.8200 - mean_absolute_error: 0.7931 - mean_squared_error: 1.8200\n",
            "Epoch 86/100\n",
            "3904/3904 [==============================] - 0s 65us/sample - loss: 1.9908 - mean_absolute_error: 0.8191 - mean_squared_error: 1.9908\n",
            "Epoch 87/100\n",
            "3904/3904 [==============================] - 0s 65us/sample - loss: 1.7722 - mean_absolute_error: 0.7728 - mean_squared_error: 1.7722\n",
            "Epoch 88/100\n",
            "3904/3904 [==============================] - 0s 67us/sample - loss: 2.0000 - mean_absolute_error: 0.8437 - mean_squared_error: 2.0000\n",
            "Epoch 89/100\n",
            "3904/3904 [==============================] - 0s 65us/sample - loss: 1.7797 - mean_absolute_error: 0.7563 - mean_squared_error: 1.7797\n",
            "Epoch 90/100\n",
            "3904/3904 [==============================] - 0s 65us/sample - loss: 1.8483 - mean_absolute_error: 0.7758 - mean_squared_error: 1.8483\n",
            "Epoch 91/100\n",
            "3904/3904 [==============================] - 0s 64us/sample - loss: 1.9518 - mean_absolute_error: 0.8184 - mean_squared_error: 1.9518\n",
            "Epoch 92/100\n",
            "3904/3904 [==============================] - 0s 65us/sample - loss: 1.8584 - mean_absolute_error: 0.7598 - mean_squared_error: 1.8584\n",
            "Epoch 93/100\n",
            "3904/3904 [==============================] - 0s 65us/sample - loss: 1.9048 - mean_absolute_error: 0.7949 - mean_squared_error: 1.9048\n",
            "Epoch 94/100\n",
            "3904/3904 [==============================] - 0s 66us/sample - loss: 1.7508 - mean_absolute_error: 0.7663 - mean_squared_error: 1.7508\n",
            "Epoch 95/100\n",
            "3904/3904 [==============================] - 0s 64us/sample - loss: 1.8661 - mean_absolute_error: 0.7721 - mean_squared_error: 1.8661\n",
            "Epoch 96/100\n",
            "3904/3904 [==============================] - 0s 65us/sample - loss: 1.9289 - mean_absolute_error: 0.8271 - mean_squared_error: 1.9289\n",
            "Epoch 97/100\n",
            "3904/3904 [==============================] - 0s 65us/sample - loss: 1.7894 - mean_absolute_error: 0.7679 - mean_squared_error: 1.7894\n",
            "Epoch 98/100\n",
            "3904/3904 [==============================] - 0s 66us/sample - loss: 1.9130 - mean_absolute_error: 0.8037 - mean_squared_error: 1.9130\n",
            "Epoch 99/100\n",
            "3904/3904 [==============================] - 0s 63us/sample - loss: 1.8169 - mean_absolute_error: 0.7650 - mean_squared_error: 1.8169\n",
            "Epoch 100/100\n",
            "3904/3904 [==============================] - 0s 65us/sample - loss: 1.9651 - mean_absolute_error: 0.8103 - mean_squared_error: 1.9651\n",
            "976/976 [==============================] - 1s 770us/sample - loss: 2.1472 - mean_absolute_error: 0.8272 - mean_squared_error: 2.1472\n",
            "[[176.64754]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zk-8aYehfqTp",
        "colab_type": "code",
        "outputId": "1c56db99-3016-4da1-d75c-4e139efee170",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3451
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow.keras as ks\n",
        "\n",
        "\n",
        "\n",
        "layers = [\n",
        "    ks.layers.InputLayer(input_shape = (len(X_train[1]), )),\n",
        "    ks.layers.Dense(units = 128, activation = ks.activations.relu),\n",
        "    #ks.layers.Dropout(0.25),\n",
        "    ks.layers.Dense(units = 64, activation = ks.activations.relu),\n",
        "    #ks.layers.Dropout(0.25),\n",
        "    ks.layers.Dense(units = 1)\n",
        "]\n",
        "model2 = ks.Sequential(layers)\n",
        "\n",
        "model2.compile(optimizer = ks.optimizers.Adam(),\n",
        "              loss = ks.losses.MeanSquaredError(),\n",
        "              metrics = [ks.metrics.MeanAbsoluteError(), ks.metrics.MeanSquaredError()]\n",
        ")\n",
        "\n",
        "model2.fit(x = X_train, y = y_train, batch_size = 20, epochs = 100)\n",
        "model2.evaluate(x = X_test, y = y_test, batch_size = 1)\n",
        "\n",
        "prediction1 = model2.predict(X_lately, verbose = 0)\n",
        "\n",
        "print(prediction1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "3904/3904 [==============================] - 0s 84us/sample - loss: 2619.2587 - mean_absolute_error: 29.9986 - mean_squared_error: 2619.2581\n",
            "Epoch 2/100\n",
            "3904/3904 [==============================] - 0s 66us/sample - loss: 87.0546 - mean_absolute_error: 6.9298 - mean_squared_error: 87.0546\n",
            "Epoch 3/100\n",
            "3904/3904 [==============================] - 0s 69us/sample - loss: 30.6904 - mean_absolute_error: 4.1713 - mean_squared_error: 30.6904\n",
            "Epoch 4/100\n",
            "3904/3904 [==============================] - 0s 65us/sample - loss: 13.2954 - mean_absolute_error: 2.7019 - mean_squared_error: 13.2954\n",
            "Epoch 5/100\n",
            "3904/3904 [==============================] - 0s 69us/sample - loss: 7.5503 - mean_absolute_error: 2.0015 - mean_squared_error: 7.5503\n",
            "Epoch 6/100\n",
            "3904/3904 [==============================] - 0s 66us/sample - loss: 4.6522 - mean_absolute_error: 1.5367 - mean_squared_error: 4.6522\n",
            "Epoch 7/100\n",
            "3904/3904 [==============================] - 0s 64us/sample - loss: 3.4853 - mean_absolute_error: 1.2874 - mean_squared_error: 3.4853\n",
            "Epoch 8/100\n",
            "3904/3904 [==============================] - 0s 69us/sample - loss: 2.9580 - mean_absolute_error: 1.1540 - mean_squared_error: 2.9580\n",
            "Epoch 9/100\n",
            "3904/3904 [==============================] - 0s 67us/sample - loss: 2.6656 - mean_absolute_error: 1.0724 - mean_squared_error: 2.6656\n",
            "Epoch 10/100\n",
            "3904/3904 [==============================] - 0s 68us/sample - loss: 2.3306 - mean_absolute_error: 0.9794 - mean_squared_error: 2.3306\n",
            "Epoch 11/100\n",
            "3904/3904 [==============================] - 0s 66us/sample - loss: 2.2701 - mean_absolute_error: 0.9479 - mean_squared_error: 2.2701\n",
            "Epoch 12/100\n",
            "3904/3904 [==============================] - 0s 66us/sample - loss: 2.1303 - mean_absolute_error: 0.9120 - mean_squared_error: 2.1303\n",
            "Epoch 13/100\n",
            "3904/3904 [==============================] - 0s 68us/sample - loss: 2.2458 - mean_absolute_error: 0.9118 - mean_squared_error: 2.2458\n",
            "Epoch 14/100\n",
            "3904/3904 [==============================] - 0s 67us/sample - loss: 2.0515 - mean_absolute_error: 0.8746 - mean_squared_error: 2.0515\n",
            "Epoch 15/100\n",
            "3904/3904 [==============================] - 0s 65us/sample - loss: 2.0269 - mean_absolute_error: 0.8844 - mean_squared_error: 2.0269\n",
            "Epoch 16/100\n",
            "3904/3904 [==============================] - 0s 65us/sample - loss: 1.9772 - mean_absolute_error: 0.8419 - mean_squared_error: 1.9772\n",
            "Epoch 17/100\n",
            "3904/3904 [==============================] - 0s 66us/sample - loss: 2.0971 - mean_absolute_error: 0.8643 - mean_squared_error: 2.0971\n",
            "Epoch 18/100\n",
            "3904/3904 [==============================] - 0s 67us/sample - loss: 1.9443 - mean_absolute_error: 0.8343 - mean_squared_error: 1.9443\n",
            "Epoch 19/100\n",
            "3904/3904 [==============================] - 0s 68us/sample - loss: 2.0563 - mean_absolute_error: 0.8564 - mean_squared_error: 2.0563\n",
            "Epoch 20/100\n",
            "3904/3904 [==============================] - 0s 65us/sample - loss: 2.0696 - mean_absolute_error: 0.8622 - mean_squared_error: 2.0696\n",
            "Epoch 21/100\n",
            "3904/3904 [==============================] - 0s 68us/sample - loss: 2.1889 - mean_absolute_error: 0.8809 - mean_squared_error: 2.1889\n",
            "Epoch 22/100\n",
            "3904/3904 [==============================] - 0s 67us/sample - loss: 2.0375 - mean_absolute_error: 0.8356 - mean_squared_error: 2.0375\n",
            "Epoch 23/100\n",
            "3904/3904 [==============================] - 0s 69us/sample - loss: 2.2640 - mean_absolute_error: 0.8893 - mean_squared_error: 2.2640\n",
            "Epoch 24/100\n",
            "3904/3904 [==============================] - 0s 68us/sample - loss: 1.9835 - mean_absolute_error: 0.8151 - mean_squared_error: 1.9835\n",
            "Epoch 25/100\n",
            "3904/3904 [==============================] - 0s 65us/sample - loss: 1.9500 - mean_absolute_error: 0.8410 - mean_squared_error: 1.9500\n",
            "Epoch 26/100\n",
            "3904/3904 [==============================] - 0s 70us/sample - loss: 1.9224 - mean_absolute_error: 0.8147 - mean_squared_error: 1.9224\n",
            "Epoch 27/100\n",
            "3904/3904 [==============================] - 0s 69us/sample - loss: 2.0500 - mean_absolute_error: 0.8411 - mean_squared_error: 2.0500\n",
            "Epoch 28/100\n",
            "3904/3904 [==============================] - 0s 68us/sample - loss: 2.0260 - mean_absolute_error: 0.8257 - mean_squared_error: 2.0260\n",
            "Epoch 29/100\n",
            "3904/3904 [==============================] - 0s 70us/sample - loss: 2.1739 - mean_absolute_error: 0.8898 - mean_squared_error: 2.1739\n",
            "Epoch 30/100\n",
            "3904/3904 [==============================] - 0s 67us/sample - loss: 1.8759 - mean_absolute_error: 0.8027 - mean_squared_error: 1.8759\n",
            "Epoch 31/100\n",
            "3904/3904 [==============================] - 0s 67us/sample - loss: 1.9800 - mean_absolute_error: 0.8370 - mean_squared_error: 1.9800\n",
            "Epoch 32/100\n",
            "3904/3904 [==============================] - 0s 68us/sample - loss: 2.1628 - mean_absolute_error: 0.8630 - mean_squared_error: 2.1628\n",
            "Epoch 33/100\n",
            "3904/3904 [==============================] - 0s 68us/sample - loss: 2.0349 - mean_absolute_error: 0.8301 - mean_squared_error: 2.0349\n",
            "Epoch 34/100\n",
            "3904/3904 [==============================] - 0s 67us/sample - loss: 1.9803 - mean_absolute_error: 0.8086 - mean_squared_error: 1.9803\n",
            "Epoch 35/100\n",
            "3904/3904 [==============================] - 0s 67us/sample - loss: 1.9649 - mean_absolute_error: 0.8144 - mean_squared_error: 1.9649\n",
            "Epoch 36/100\n",
            "3904/3904 [==============================] - 0s 68us/sample - loss: 1.9315 - mean_absolute_error: 0.8026 - mean_squared_error: 1.9315\n",
            "Epoch 37/100\n",
            "3904/3904 [==============================] - 0s 68us/sample - loss: 2.2438 - mean_absolute_error: 0.9185 - mean_squared_error: 2.2438\n",
            "Epoch 38/100\n",
            "3904/3904 [==============================] - 0s 67us/sample - loss: 2.2516 - mean_absolute_error: 0.8860 - mean_squared_error: 2.2516\n",
            "Epoch 39/100\n",
            "3904/3904 [==============================] - 0s 67us/sample - loss: 1.9151 - mean_absolute_error: 0.8085 - mean_squared_error: 1.9151\n",
            "Epoch 40/100\n",
            "3904/3904 [==============================] - 0s 68us/sample - loss: 2.2380 - mean_absolute_error: 0.8545 - mean_squared_error: 2.2380\n",
            "Epoch 41/100\n",
            "3904/3904 [==============================] - 0s 65us/sample - loss: 1.9525 - mean_absolute_error: 0.7918 - mean_squared_error: 1.9525\n",
            "Epoch 42/100\n",
            "3904/3904 [==============================] - 0s 68us/sample - loss: 1.9936 - mean_absolute_error: 0.8101 - mean_squared_error: 1.9936\n",
            "Epoch 43/100\n",
            "3904/3904 [==============================] - 0s 69us/sample - loss: 1.9177 - mean_absolute_error: 0.8084 - mean_squared_error: 1.9177\n",
            "Epoch 44/100\n",
            "3904/3904 [==============================] - 0s 66us/sample - loss: 1.8796 - mean_absolute_error: 0.7960 - mean_squared_error: 1.8796\n",
            "Epoch 45/100\n",
            "3904/3904 [==============================] - 0s 66us/sample - loss: 1.9704 - mean_absolute_error: 0.8221 - mean_squared_error: 1.9704\n",
            "Epoch 46/100\n",
            "3904/3904 [==============================] - 0s 68us/sample - loss: 1.9812 - mean_absolute_error: 0.8154 - mean_squared_error: 1.9812\n",
            "Epoch 47/100\n",
            "3904/3904 [==============================] - 0s 66us/sample - loss: 2.1411 - mean_absolute_error: 0.8553 - mean_squared_error: 2.1411\n",
            "Epoch 48/100\n",
            "3904/3904 [==============================] - 0s 68us/sample - loss: 2.0839 - mean_absolute_error: 0.8495 - mean_squared_error: 2.0839\n",
            "Epoch 49/100\n",
            "3904/3904 [==============================] - 0s 67us/sample - loss: 1.8707 - mean_absolute_error: 0.8011 - mean_squared_error: 1.8707\n",
            "Epoch 50/100\n",
            "3904/3904 [==============================] - 0s 65us/sample - loss: 1.9408 - mean_absolute_error: 0.8208 - mean_squared_error: 1.9408\n",
            "Epoch 51/100\n",
            "3904/3904 [==============================] - 0s 67us/sample - loss: 1.8264 - mean_absolute_error: 0.7975 - mean_squared_error: 1.8264\n",
            "Epoch 52/100\n",
            "3904/3904 [==============================] - 0s 65us/sample - loss: 2.0475 - mean_absolute_error: 0.8509 - mean_squared_error: 2.0475\n",
            "Epoch 53/100\n",
            "3904/3904 [==============================] - 0s 66us/sample - loss: 2.0138 - mean_absolute_error: 0.8326 - mean_squared_error: 2.0138\n",
            "Epoch 54/100\n",
            "3904/3904 [==============================] - 0s 69us/sample - loss: 2.3352 - mean_absolute_error: 0.8991 - mean_squared_error: 2.3352\n",
            "Epoch 55/100\n",
            "3904/3904 [==============================] - 0s 65us/sample - loss: 1.9374 - mean_absolute_error: 0.8407 - mean_squared_error: 1.9374\n",
            "Epoch 56/100\n",
            "3904/3904 [==============================] - 0s 67us/sample - loss: 1.9279 - mean_absolute_error: 0.7851 - mean_squared_error: 1.9279\n",
            "Epoch 57/100\n",
            "3904/3904 [==============================] - 0s 65us/sample - loss: 1.9432 - mean_absolute_error: 0.8318 - mean_squared_error: 1.9432\n",
            "Epoch 58/100\n",
            "3904/3904 [==============================] - 0s 68us/sample - loss: 1.9510 - mean_absolute_error: 0.8151 - mean_squared_error: 1.9510\n",
            "Epoch 59/100\n",
            "3904/3904 [==============================] - 0s 67us/sample - loss: 2.1746 - mean_absolute_error: 0.8520 - mean_squared_error: 2.1746\n",
            "Epoch 60/100\n",
            "3904/3904 [==============================] - 0s 68us/sample - loss: 2.0082 - mean_absolute_error: 0.8194 - mean_squared_error: 2.0082\n",
            "Epoch 61/100\n",
            "3904/3904 [==============================] - 0s 65us/sample - loss: 1.9053 - mean_absolute_error: 0.8002 - mean_squared_error: 1.9053\n",
            "Epoch 62/100\n",
            "3904/3904 [==============================] - 0s 68us/sample - loss: 1.9437 - mean_absolute_error: 0.8290 - mean_squared_error: 1.9437\n",
            "Epoch 63/100\n",
            "3904/3904 [==============================] - 0s 67us/sample - loss: 2.1500 - mean_absolute_error: 0.8512 - mean_squared_error: 2.1500\n",
            "Epoch 64/100\n",
            "3904/3904 [==============================] - 0s 68us/sample - loss: 2.4014 - mean_absolute_error: 0.9523 - mean_squared_error: 2.4014\n",
            "Epoch 65/100\n",
            "3904/3904 [==============================] - 0s 65us/sample - loss: 2.2398 - mean_absolute_error: 0.8880 - mean_squared_error: 2.2398\n",
            "Epoch 66/100\n",
            "3904/3904 [==============================] - 0s 68us/sample - loss: 1.9045 - mean_absolute_error: 0.7987 - mean_squared_error: 1.9045\n",
            "Epoch 67/100\n",
            "3904/3904 [==============================] - 0s 67us/sample - loss: 2.1751 - mean_absolute_error: 0.8588 - mean_squared_error: 2.1751\n",
            "Epoch 68/100\n",
            "3904/3904 [==============================] - 0s 65us/sample - loss: 1.9030 - mean_absolute_error: 0.8014 - mean_squared_error: 1.9030\n",
            "Epoch 69/100\n",
            "3904/3904 [==============================] - 0s 67us/sample - loss: 1.8559 - mean_absolute_error: 0.7894 - mean_squared_error: 1.8559\n",
            "Epoch 70/100\n",
            "3904/3904 [==============================] - 0s 65us/sample - loss: 2.0722 - mean_absolute_error: 0.8249 - mean_squared_error: 2.0722\n",
            "Epoch 71/100\n",
            "3904/3904 [==============================] - 0s 68us/sample - loss: 2.0001 - mean_absolute_error: 0.8242 - mean_squared_error: 2.0001\n",
            "Epoch 72/100\n",
            "3904/3904 [==============================] - 0s 68us/sample - loss: 1.9895 - mean_absolute_error: 0.8019 - mean_squared_error: 1.9895\n",
            "Epoch 73/100\n",
            "3904/3904 [==============================] - 0s 67us/sample - loss: 1.8497 - mean_absolute_error: 0.7928 - mean_squared_error: 1.8497\n",
            "Epoch 74/100\n",
            "3904/3904 [==============================] - 0s 68us/sample - loss: 1.9697 - mean_absolute_error: 0.8154 - mean_squared_error: 1.9697\n",
            "Epoch 75/100\n",
            "3904/3904 [==============================] - 0s 69us/sample - loss: 1.8768 - mean_absolute_error: 0.8048 - mean_squared_error: 1.8768\n",
            "Epoch 76/100\n",
            "3904/3904 [==============================] - 0s 69us/sample - loss: 2.0566 - mean_absolute_error: 0.8179 - mean_squared_error: 2.0566\n",
            "Epoch 77/100\n",
            "3904/3904 [==============================] - 0s 68us/sample - loss: 2.0512 - mean_absolute_error: 0.8388 - mean_squared_error: 2.0512\n",
            "Epoch 78/100\n",
            "3904/3904 [==============================] - 0s 68us/sample - loss: 2.1718 - mean_absolute_error: 0.8884 - mean_squared_error: 2.1718\n",
            "Epoch 79/100\n",
            "3904/3904 [==============================] - 0s 65us/sample - loss: 1.8719 - mean_absolute_error: 0.8069 - mean_squared_error: 1.8719\n",
            "Epoch 80/100\n",
            "3904/3904 [==============================] - 0s 69us/sample - loss: 1.9759 - mean_absolute_error: 0.8046 - mean_squared_error: 1.9759\n",
            "Epoch 81/100\n",
            "3904/3904 [==============================] - 0s 67us/sample - loss: 2.0122 - mean_absolute_error: 0.7994 - mean_squared_error: 2.0122\n",
            "Epoch 82/100\n",
            "3904/3904 [==============================] - 0s 69us/sample - loss: 1.9309 - mean_absolute_error: 0.8030 - mean_squared_error: 1.9309\n",
            "Epoch 83/100\n",
            "3904/3904 [==============================] - 0s 66us/sample - loss: 1.9895 - mean_absolute_error: 0.8298 - mean_squared_error: 1.9895\n",
            "Epoch 84/100\n",
            "3904/3904 [==============================] - 0s 65us/sample - loss: 1.9565 - mean_absolute_error: 0.8290 - mean_squared_error: 1.9565\n",
            "Epoch 85/100\n",
            "3904/3904 [==============================] - 0s 68us/sample - loss: 1.9216 - mean_absolute_error: 0.8135 - mean_squared_error: 1.9216\n",
            "Epoch 86/100\n",
            "3904/3904 [==============================] - 0s 70us/sample - loss: 2.1755 - mean_absolute_error: 0.8770 - mean_squared_error: 2.1755\n",
            "Epoch 87/100\n",
            "3904/3904 [==============================] - 0s 67us/sample - loss: 2.0398 - mean_absolute_error: 0.8010 - mean_squared_error: 2.0398\n",
            "Epoch 88/100\n",
            "3904/3904 [==============================] - 0s 66us/sample - loss: 1.9112 - mean_absolute_error: 0.8346 - mean_squared_error: 1.9112\n",
            "Epoch 89/100\n",
            "3904/3904 [==============================] - 0s 66us/sample - loss: 2.0495 - mean_absolute_error: 0.8035 - mean_squared_error: 2.0495\n",
            "Epoch 90/100\n",
            "3904/3904 [==============================] - 0s 70us/sample - loss: 1.9536 - mean_absolute_error: 0.8065 - mean_squared_error: 1.9536\n",
            "Epoch 91/100\n",
            "3904/3904 [==============================] - 0s 67us/sample - loss: 1.9120 - mean_absolute_error: 0.7865 - mean_squared_error: 1.9120\n",
            "Epoch 92/100\n",
            "3904/3904 [==============================] - 0s 66us/sample - loss: 1.8842 - mean_absolute_error: 0.7936 - mean_squared_error: 1.8842\n",
            "Epoch 93/100\n",
            "3904/3904 [==============================] - 0s 67us/sample - loss: 2.0288 - mean_absolute_error: 0.8284 - mean_squared_error: 2.0288\n",
            "Epoch 94/100\n",
            "3904/3904 [==============================] - 0s 68us/sample - loss: 1.9612 - mean_absolute_error: 0.8266 - mean_squared_error: 1.9612\n",
            "Epoch 95/100\n",
            "3904/3904 [==============================] - 0s 66us/sample - loss: 1.9505 - mean_absolute_error: 0.8284 - mean_squared_error: 1.9505\n",
            "Epoch 96/100\n",
            "3904/3904 [==============================] - 0s 67us/sample - loss: 1.8579 - mean_absolute_error: 0.7837 - mean_squared_error: 1.8579\n",
            "Epoch 97/100\n",
            "3904/3904 [==============================] - 0s 67us/sample - loss: 1.9833 - mean_absolute_error: 0.8401 - mean_squared_error: 1.9833\n",
            "Epoch 98/100\n",
            "3904/3904 [==============================] - 0s 68us/sample - loss: 1.9895 - mean_absolute_error: 0.8283 - mean_squared_error: 1.9895\n",
            "Epoch 99/100\n",
            "3904/3904 [==============================] - 0s 67us/sample - loss: 1.9862 - mean_absolute_error: 0.8152 - mean_squared_error: 1.9862\n",
            "Epoch 100/100\n",
            "3904/3904 [==============================] - 0s 66us/sample - loss: 2.0050 - mean_absolute_error: 0.8028 - mean_squared_error: 2.0050\n",
            "976/976 [==============================] - 1s 745us/sample - loss: 2.0766 - mean_absolute_error: 0.8093 - mean_squared_error: 2.0766\n",
            "[[177.42516]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Y3loQMHTktQ",
        "colab_type": "code",
        "outputId": "de74a066-42d3-4bc4-8498-f27b5546db18",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "'''\n",
        "AAPL\n",
        "today close:[[179.81609]] or [[181.13953]] (+/- 2) actual 178.97\n",
        "\n",
        "tomorrow: [178.4155]\n",
        "\n",
        "AMZN\n",
        "today close [[1837.4268]] or [[1809.4828]] or [[1823.4548]](+/- 127)\n",
        "\n",
        "GOOG\n",
        "today close [[1131.2982]] or [[1132.281]] or [[1137.1511]](+/- 73)\n",
        "\n",
        "NFLX\n",
        "today close: 359.5414 (+/- 9.5)\n",
        "\n",
        "ROKU\n",
        "today close: [[97.16571]] (+/- 4.7)\n",
        "\n",
        "\n",
        "S&P 500 (^GSPC)\n",
        "TODAY CLOSE: [[2824.498]], CLOSE TODAY ACTUAL: 2826.06 \n",
        "'''\n",
        "\n",
        "\n",
        "'''try giving the open prices and make it predict the close, could be cool idk. like only using \n",
        "close as a label and nothing more, close wouldnt be in the data at all'''\n",
        "\n",
        "print((prediction + prediction1)/2)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[177.03635]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1UbiSfYiTTIl",
        "colab_type": "text"
      },
      "source": [
        "[[179.37996]\n",
        " [178.4276 ]\n",
        " [170.96571]\n",
        " [175.49965]\n",
        " [171.13963]\n",
        " [166.89964]\n",
        " [167.77129]]\n",
        " \n",
        " prices for next 7 days\n",
        " we shall see\n",
        " appl"
      ]
    }
  ]
}